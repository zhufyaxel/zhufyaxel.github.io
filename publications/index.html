<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Fengyuan Zhu's Research Space </title> <meta name="author" content="Fengyuan Zhu"> <meta name="description" content="This is the website to show Fengyuan's research process. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="stylesheet" href="/assets/css/video.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zhufyaxel.github.io/publications/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Fengyuan Zhu's Research Space </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-4 abbr"> <figure> <div class="video-container" data-zoomable="" data-video-path="/assets/video/publication_preview/BeyondThePhone.mp4"> <video class="preview z-depth-1 rounded" width="100%" height="auto" autoplay="" muted="" loop="" playsinline="" preload="auto" loading="eager" poster="/assets/video/publication_preview/BeyondThePhone.jpg" onerror="this.style.display='none'; document.getElementById('fallback-assets-video-publication-preview-beyondthephone-mp4').style.display='block';"> <source src="/assets/video/publication_preview/BeyondThePhone.mp4" type="video/mp4"></source> <source src="/assets/video/publication_preview/BeyondThePhone.webm" type="video/webm"></source> Your browser does not support the video tag. </video> <div class="video-play-overlay"> <div class="play-button-large"></div> </div> <div id="fallback-assets-video-publication-preview-beyondthephone-mp4" class="video-fallback" style="display:none;" data-video-path="/assets/video/publication_preview/BeyondThePhone.mp4"> <img src="/assets/video/publication_preview/BeyondThePhone.jpg" alt="Video preview image" class="preview z-depth-1 rounded"> <div class="play-button-large"></div> </div> </div> </figure> <style>.video-container{position:relative;width:100%;cursor:pointer;overflow:hidden;border-radius:4px;transition:transform .3s ease,box-shadow .3s ease}.video-container:hover{transform:scale(1.01);box-shadow:0 4px 8px rgba(0,0,0,0.1)}.video-container video{display:block;max-width:100%}.video-play-overlay{position:absolute;top:0;left:0;width:100%;height:100%;background-color:rgba(0,0,0,0.1);opacity:0;transition:opacity .3s ease;display:flex;align-items:center;justify-content:center}.video-container:hover .video-play-overlay{opacity:1}.video-fallback{position:relative;width:100%;cursor:pointer}.video-fallback img{width:100%;height:auto;display:block;border-radius:4px}.play-button-large{width:80px;height:80px;background-color:rgba(0,0,0,0.6);border-radius:50%;display:flex;align-items:center;justify-content:center;transition:background-color .3s ease,transform .3s ease}.play-button-large:before{content:'';display:block;width:0;height:0;border-top:15px solid transparent;border-bottom:15px solid transparent;border-left:25px solid white;margin-left:5px}.video-container:hover .play-button-large,.video-fallback:hover .play-button-large{background-color:rgba(0,0,0,0.8);transform:scale(1.1)}</style> </div> <div id="BeyondThePhone" class="col-sm-8"> <div class="title">Beyond the Phone: Exploring Phone-XR Integration through Multi-View Transitions for Real-World Applications</div> <div class="author"> <em>Fengyuan Zhu</em>, Xun Qian , Daniel Kalmar , Mahdi Tayarani , Eric J. Gonzalez , Mar Gonzalez-Franco , David Kim , and Ruofei Du </div> <div class="periodical"> <em>In 2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)</em> , Saint Malo, France, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/IEEEVR25_Beyond_the_Phone.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://youtu.be/vSjj8C-d4qo?si=TEG0mgeMj86UVWoP" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1109/VR59515.2025.00099" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Despite the growing prevalence of Extended Reality (XR) headsets, their integration with mobile phones remains limited. Existing approaches primarily replicate the phone’s interface in XR or use the phone solely as a 6DOF controller. This paper introduces a novel framework for seamless transitions among mirrored, magnified, and augmented views, dynamically adapts the interface with the content and state of mobile applications. To achieve this, we establish a design space through literature reviews and expert workshops, outline user journeys with common real-world applications, and develop a prototype system that automatically analyzes UI layouts to provide enhanced controls and spatial augmentation. We validate our prototype system with a user study to assess its adaptability to a broad spectrum of applications at runtime, reported its strengths and weaknesses, and suggest directions to advance the future adaption in Phone-XR integration.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-4 abbr"> <figure> <div class="video-container" data-zoomable="" data-video-path="/assets/video/publication_preview/PhoneInVR.mp4"> <video class="preview z-depth-1 rounded" width="100%" height="auto" autoplay="" muted="" loop="" playsinline="" preload="auto" loading="eager" poster="/assets/video/publication_preview/PhoneInVR.jpg" onerror="this.style.display='none'; document.getElementById('fallback-assets-video-publication-preview-phoneinvr-mp4').style.display='block';"> <source src="/assets/video/publication_preview/PhoneInVR.mp4" type="video/mp4"></source> <source src="/assets/video/publication_preview/PhoneInVR.webm" type="video/webm"></source> Your browser does not support the video tag. </video> <div class="video-play-overlay"> <div class="play-button-large"></div> </div> <div id="fallback-assets-video-publication-preview-phoneinvr-mp4" class="video-fallback" style="display:none;" data-video-path="/assets/video/publication_preview/PhoneInVR.mp4"> <img src="/assets/video/publication_preview/PhoneInVR.jpg" alt="Video preview image" class="preview z-depth-1 rounded"> <div class="play-button-large"></div> </div> </div> </figure> <style>.video-container{position:relative;width:100%;cursor:pointer;overflow:hidden;border-radius:4px;transition:transform .3s ease,box-shadow .3s ease}.video-container:hover{transform:scale(1.01);box-shadow:0 4px 8px rgba(0,0,0,0.1)}.video-container video{display:block;max-width:100%}.video-play-overlay{position:absolute;top:0;left:0;width:100%;height:100%;background-color:rgba(0,0,0,0.1);opacity:0;transition:opacity .3s ease;display:flex;align-items:center;justify-content:center}.video-container:hover .video-play-overlay{opacity:1}.video-fallback{position:relative;width:100%;cursor:pointer}.video-fallback img{width:100%;height:auto;display:block;border-radius:4px}.play-button-large{width:80px;height:80px;background-color:rgba(0,0,0,0.6);border-radius:50%;display:flex;align-items:center;justify-content:center;transition:background-color .3s ease,transform .3s ease}.play-button-large:before{content:'';display:block;width:0;height:0;border-top:15px solid transparent;border-bottom:15px solid transparent;border-left:25px solid white;margin-left:5px}.video-container:hover .play-button-large,.video-fallback:hover .play-button-large{background-color:rgba(0,0,0,0.8);transform:scale(1.1)}</style> </div> <div id="PhoneInVR" class="col-sm-8"> <div class="title">PhoneInVR: An Evaluation of Spatial Anchoring and Interaction Techniques for Smartphone Usage in Virtual Reality</div> <div class="author"> <em>Fengyuan Zhu</em>, Mauricio Sousa , Ludwig Sidenmark , and Tovi Grossman </div> <div class="periodical"> <em>In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems</em> , Honolulu, HI, USA, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/PhoneInVR.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=pQN98TbhAKM" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>When users wear a virtual reality (VR) headset, they lose access to their smartphone and accompanying apps. Past work has proposed smartphones as enhanced VR controllers, but little work has explored using existing smartphone apps and performing traditional smartphone interactions while in VR. In this paper, we consider three potential spatial anchorings for rendering smartphones in VR: On top of a tracked physical smartphone which the user holds (Phone-locked), on top of the users empty hand, as if holding a virtual smartphone (Hand-locked), or in a static position in front of the user (World-locked). We conducted a comparative study of target acquisition, swiping, and scrolling tasks across these anchorings using direct Touch or above-the-surface Pinch. Our findings indicate that physically holding a smartphone with Touch improves accuracy and speed for all tasks, and Pinch performed better with virtual smartphones. These findings provide a valuable foundation to enable smartphones in VR. </p> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-4 abbr"> <figure> <div class="video-container" data-zoomable="" data-video-path="/assets/video/publication_preview/PinchLens.mp4"> <video class="preview z-depth-1 rounded" width="100%" height="auto" autoplay="" muted="" loop="" playsinline="" preload="auto" loading="eager" poster="/assets/video/publication_preview/PinchLens.jpg" onerror="this.style.display='none'; document.getElementById('fallback-assets-video-publication-preview-pinchlens-mp4').style.display='block';"> <source src="/assets/video/publication_preview/PinchLens.mp4" type="video/mp4"></source> <source src="/assets/video/publication_preview/PinchLens.webm" type="video/webm"></source> Your browser does not support the video tag. </video> <div class="video-play-overlay"> <div class="play-button-large"></div> </div> <div id="fallback-assets-video-publication-preview-pinchlens-mp4" class="video-fallback" style="display:none;" data-video-path="/assets/video/publication_preview/PinchLens.mp4"> <img src="/assets/video/publication_preview/PinchLens.jpg" alt="Video preview image" class="preview z-depth-1 rounded"> <div class="play-button-large"></div> </div> </div> </figure> <style>.video-container{position:relative;width:100%;cursor:pointer;overflow:hidden;border-radius:4px;transition:transform .3s ease,box-shadow .3s ease}.video-container:hover{transform:scale(1.01);box-shadow:0 4px 8px rgba(0,0,0,0.1)}.video-container video{display:block;max-width:100%}.video-play-overlay{position:absolute;top:0;left:0;width:100%;height:100%;background-color:rgba(0,0,0,0.1);opacity:0;transition:opacity .3s ease;display:flex;align-items:center;justify-content:center}.video-container:hover .video-play-overlay{opacity:1}.video-fallback{position:relative;width:100%;cursor:pointer}.video-fallback img{width:100%;height:auto;display:block;border-radius:4px}.play-button-large{width:80px;height:80px;background-color:rgba(0,0,0,0.6);border-radius:50%;display:flex;align-items:center;justify-content:center;transition:background-color .3s ease,transform .3s ease}.play-button-large:before{content:'';display:block;width:0;height:0;border-top:15px solid transparent;border-bottom:15px solid transparent;border-left:25px solid white;margin-left:5px}.video-container:hover .play-button-large,.video-fallback:hover .play-button-large{background-color:rgba(0,0,0,0.8);transform:scale(1.1)}</style> </div> <div id="PinchLens" class="col-sm-8"> <div class="title">PinchLens: Applying Spatial Magnification and Adaptive Control-Display Gain for Precise Selection in Virtual Reality</div> <div class="author"> <em>Fengyuan Zhu</em>, Ludwig Sidenmark , Mauricio Sousa , and Tovi Grossman </div> <div class="periodical"> <em>In 2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</em> , Sydney, New South Wales, Australia, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/PinchLens.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://youtu.be/MAlkiWVsMeQ?si=zPMXPQNwDKb0Yoc5" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>We present PinchLens, a new free-hand target selection technique for acquiring small and dense targets in Virtual Reality. Traditional pinch-based selection does not allow people to precisely manipulate small and dense objects effectively due to tracking and perceptual inaccuracies. Our approach combines spatial magnification, an adaptive control-display gain, and visual feedback to improve selection accuracy. When a user starts the pinching selection process, a magnifying bubble expands the scale of nearby targets, an adaptive control-to-display ratio is applied to the user‘s hand for precision, and a cursor is displayed at the estimated pinch point for enhanced visual feedback. We performed a user study to compare our technique to traditional pinch selection and several variations to isolate the impact of each of the technique’s features. The results showed that PinchLens significantly outperformed traditional pinch selection, reducing error rates from 18.9% to 1.9%. Furthermore, we found that magnification was the dominant feature to produce this improvement, while the adaptive control-display gain and visual cursor of pinch were also helpful in several conditions.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-4 abbr"> <figure> <div class="video-container" data-zoomable="" data-video-path="/assets/video/publication_preview/TouchTheDroid.mp4"> <video class="preview z-depth-1 rounded" width="100%" height="auto" autoplay="" muted="" loop="" playsinline="" preload="auto" loading="eager" poster="/assets/video/publication_preview/TouchTheDroid.jpg" onerror="this.style.display='none'; document.getElementById('fallback-assets-video-publication-preview-touchthedroid-mp4').style.display='block';"> <source src="/assets/video/publication_preview/TouchTheDroid.mp4" type="video/mp4"></source> <source src="/assets/video/publication_preview/TouchTheDroid.webm" type="video/webm"></source> Your browser does not support the video tag. </video> <div class="video-play-overlay"> <div class="play-button-large"></div> </div> <div id="fallback-assets-video-publication-preview-touchthedroid-mp4" class="video-fallback" style="display:none;" data-video-path="/assets/video/publication_preview/TouchTheDroid.mp4"> <img src="/assets/video/publication_preview/TouchTheDroid.jpg" alt="Video preview image" class="preview z-depth-1 rounded"> <div class="play-button-large"></div> </div> </div> </figure> <style>.video-container{position:relative;width:100%;cursor:pointer;overflow:hidden;border-radius:4px;transition:transform .3s ease,box-shadow .3s ease}.video-container:hover{transform:scale(1.01);box-shadow:0 4px 8px rgba(0,0,0,0.1)}.video-container video{display:block;max-width:100%}.video-play-overlay{position:absolute;top:0;left:0;width:100%;height:100%;background-color:rgba(0,0,0,0.1);opacity:0;transition:opacity .3s ease;display:flex;align-items:center;justify-content:center}.video-container:hover .video-play-overlay{opacity:1}.video-fallback{position:relative;width:100%;cursor:pointer}.video-fallback img{width:100%;height:auto;display:block;border-radius:4px}.play-button-large{width:80px;height:80px;background-color:rgba(0,0,0,0.6);border-radius:50%;display:flex;align-items:center;justify-content:center;transition:background-color .3s ease,transform .3s ease}.play-button-large:before{content:'';display:block;width:0;height:0;border-top:15px solid transparent;border-bottom:15px solid transparent;border-left:25px solid white;margin-left:5px}.video-container:hover .play-button-large,.video-fallback:hover .play-button-large{background-color:rgba(0,0,0,0.8);transform:scale(1.1)}</style> </div> <div id="TouchingTheDroid" class="col-sm-8"> <div class="title">Touching The Droid: Understanding and Improving Touch Precision With Mobile Devices in Virtual Reality</div> <div class="author"> <em>Fengyuan Zhu</em>, Zhuoyue Lyu , Mauricio Sousa , and Tovi Grossman </div> <div class="periodical"> <em>In 2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</em> , Singapore, Singapore, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/TouchTheDroid.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="" class="btn btn-sm z-depth-0" role="button">Video</a> </div> <div class="abstract hidden"> <p>Touch interaction with physical smartphones and tablets in Virtual Reality offers interesting opportunities for cross-device input. Unfortunately, any imprecision in the alignment of the visual representation of either the hand or device can impact the precision of touch and the realism of the experience. We first study a user‘s ability to rely solely on preoperative feedback to perform touch interaction in VR, where no rendering of the hand is provided. Results indicate that touch in VR is possible without a visual representation of the hand, but accuracy is influenced by how the device is held and the distance traveled to the target. We then introduce a dynamic calibration algorithm to minimize the offset between the physical hand and its virtual representation. In a second study, we show that this algorithm can increase touch accuracy by 43%, and minimize depth-based “screen penetration” or “floating touch” errors.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-4 abbr"> <figure> <div class="video-container" data-zoomable="" data-video-path="/assets/video/publication_preview/bishare.mp4"> <video class="preview z-depth-1 rounded" width="100%" height="auto" autoplay="" muted="" loop="" playsinline="" preload="auto" loading="eager" poster="/assets/video/publication_preview/bishare.jpg" onerror="this.style.display='none'; document.getElementById('fallback-assets-video-publication-preview-bishare-mp4').style.display='block';"> <source src="/assets/video/publication_preview/bishare.mp4" type="video/mp4"></source> <source src="/assets/video/publication_preview/bishare.webm" type="video/webm"></source> Your browser does not support the video tag. </video> <div class="video-play-overlay"> <div class="play-button-large"></div> </div> <div id="fallback-assets-video-publication-preview-bishare-mp4" class="video-fallback" style="display:none;" data-video-path="/assets/video/publication_preview/bishare.mp4"> <img src="/assets/video/publication_preview/bishare.jpg" alt="Video preview image" class="preview z-depth-1 rounded"> <div class="play-button-large"></div> </div> </div> </figure> <style>.video-container{position:relative;width:100%;cursor:pointer;overflow:hidden;border-radius:4px;transition:transform .3s ease,box-shadow .3s ease}.video-container:hover{transform:scale(1.01);box-shadow:0 4px 8px rgba(0,0,0,0.1)}.video-container video{display:block;max-width:100%}.video-play-overlay{position:absolute;top:0;left:0;width:100%;height:100%;background-color:rgba(0,0,0,0.1);opacity:0;transition:opacity .3s ease;display:flex;align-items:center;justify-content:center}.video-container:hover .video-play-overlay{opacity:1}.video-fallback{position:relative;width:100%;cursor:pointer}.video-fallback img{width:100%;height:auto;display:block;border-radius:4px}.play-button-large{width:80px;height:80px;background-color:rgba(0,0,0,0.6);border-radius:50%;display:flex;align-items:center;justify-content:center;transition:background-color .3s ease,transform .3s ease}.play-button-large:before{content:'';display:block;width:0;height:0;border-top:15px solid transparent;border-bottom:15px solid transparent;border-left:25px solid white;margin-left:5px}.video-container:hover .play-button-large,.video-fallback:hover .play-button-large{background-color:rgba(0,0,0,0.8);transform:scale(1.1)}</style> </div> <div id="BISHARE" class="col-sm-8"> <div class="title">BISHARE: Exploring Bidirectional Interactions Between Smartphones and Head-Mounted Augmented Reality</div> <div class="author"> <em>Fengyuan Zhu</em>, and Tovi Grossman </div> <div class="periodical"> <em>In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</em> , Honolulu, HI, USA, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/BISHARE.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://www.youtube.com/watch?v=hkL4cKUleNw" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1145/3313831.3376233" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>In pursuit of a future where HMD devices can be used in tandem with smartphones and other smart devices, we present BISHARE, a design space of cross-device interactions between smartphones and ARHMDs. Our design space is unique in that it is bidirectional in nature, as it examines how both the HMD can be used to enhance smartphone tasks, and how the smartphone can be used to enhance HMD tasks. We then present an interactive prototype that enables cross-device interactions across the proposed design space. A 12-participant user study demonstrates the promise of the design space and provides insights, observations, and guidance for the future.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-4 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/vins.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="vins.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="VINS" class="col-sm-8"> <div class="title">Monocular Visual-Inertial State Estimation for Mobile Augmented Reality</div> <div class="author"> Peiliang Li , Tong Qin , Botao Hu , <em>Fengyuan Zhu</em>, and Shaojie Shen </div> <div class="periodical"> <em>In 2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)</em> , 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/8115400" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Mobile phones equipped with a monocular camera and an inertial measurement unit (IMU) are ideal platforms for augmented reality (AR) applications, but the lack of direct metric distance measurement and the existence of aggressive motions pose significant challenges on the localization of the AR device. In this work, we propose a tightly-coupled, optimization-based, monocular visual-inertial state estimation for robust camera localization in complex indoor and outdoor environments. Our approach does not require any artificial markers, and is able to recover the metric scale using the monocular camera setup. The whole system is capable of online initialization without relying on any assumptions about the environment. Our tightly-coupled formulation makes it naturally robust to aggressive motions. We develop a lightweight loop closure module that is tightly integrated with the state estimator to eliminate drift. The performance of our proposed method is demonstrated via comparison against state-of-the-art visual-inertial state estimators on public datasets and real-time AR applications on mobile devices. We release our implementation on mobile devices as open source software1.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-4 abbr"> </div> <div id="Physhare" class="col-sm-8"> <div class="title">PhyShare: Sharing Physical Interaction in Virtual Reality</div> <div class="author"> Zhenyi He , <em>Fengyuan Zhu</em>, and Ken Perlin </div> <div class="periodical"> <em>In Adjunct Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology</em> , Québec City, QC, Canada, 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Physhare.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://youtu.be/i3_3WPJAi4k?si=VgMbhVC5-FrNBwxY" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>We present PhyShare, a new haptic user interface based on actuated robots. Virtual reality has recently been gaining wide adoption, and an effective haptic feedback in these scenarios can strongly support user’s sensory in bridging virtual and physical world. Since participants do not directly observe these robotic proxies, we investigate the multiple mappings between physical robots and virtual proxies that can utilize the resources needed to provide a well rounded VR experience. PhyShare bots can act either as directly touchable objects or invisible carriers of physical objects, depending on different scenarios. They also support distributed collaboration, allowing remotely located VR collaborators to share the same physical feedback.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2016</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-4 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/dis16.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dis16.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="b2b" class="col-sm-8"> <div class="title">From Breakage to Icebreaker: Inspiration for Designing Technological Support for Human-Human Interaction</div> <div class="author"> Xiaojuan Ma , Ke Fang , and <em>Fengyuan Zhu</em> </div> <div class="periodical"> <em>In Proceedings of the 2016 ACM Conference on Designing Interactive Systems</em> , Brisbane, QLD, Australia, 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/b2b.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper explores why and how accidental breakage of technologies can promote humans to interact and ultimately lead to positive behavioral, emotional, and relational change. Through a set of research activities, including meta-synthesis of daily anecdotes, design workshops, and a case study, we gain insights into what may hinder or trigger human-human communication, and propose the conceptual and actionable process of Breakage-to-Icebreaker (B2I) design. Instead of intentionally breaking a technology, B2I design embeds mechanisms into existing products and services, creating opportunities for users to interpersonally interact online and/or offline while enjoying the original features and functionalities. Finally, we envision a broader and extended use of B2I thinking in everyday design research and practices.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Fengyuan Zhu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer src="/assets/js/video_handler.js?ef9931f062c25bcc04734e5a3e8bea1b"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>