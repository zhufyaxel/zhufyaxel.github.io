@INPROCEEDINGS{BeyondThePhone,
  author={Zhu, Fengyuan and Qian, Xun and Kalmar, Daniel and Tayarani, Mahdi and Gonzalez, Eric J. and Gonzalez-Franco, Mar and Kim, David and Du, Ruofei},
  title={Beyond the Phone: Exploring Phone-XR Integration through Multi-View Transitions for Real-World Applications},
  booktitle={2025 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
  year={2025},
  pages={770-780},
  location={Saint Malo, France},
  doi={10.1109/VR59515.2025.00099},
  abstract={Despite the growing prevalence of Extended Reality (XR) headsets, their integration with mobile phones remains limited. Existing approaches primarily replicate the phone’s interface in XR or use the phone solely as a 6DOF controller. This paper introduces a novel framework for seamless transitions among mirrored, magnified, and augmented views, dynamically adapts the interface with the content and state of mobile applications. To achieve this, we establish a design space through literature reviews and expert workshops, outline user journeys with common real-world applications, and develop a prototype system that automatically analyzes UI layouts to provide enhanced controls and spatial augmentation. We validate our prototype system with a user study to assess its adaptability to a broad spectrum of applications at runtime, reported its strengths and weaknesses, and suggest directions to advance the future adaption in Phone-XR integration.},
  keywords={},
  selected={true},
  video={https://youtu.be/vSjj8C-d4qo?si=TEG0mgeMj86UVWoP},
  preview={BeyondThePhone.mp4},
  series={VR '25}
}


@inproceedings{BISHARE,
  author = {Zhu, Fengyuan and Grossman, Tovi},
  title = {BISHARE: Exploring Bidirectional Interactions Between Smartphones and Head-Mounted Augmented Reality},
  year = {2020},
  isbn = {9781450367080},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3313831.3376233},
  doi = {10.1145/3313831.3376233},
  abstract = {In pursuit of a future where HMD devices can be used in tandem with smartphones and other smart devices, we present BISHARE, a design space of cross-device interactions between smartphones and ARHMDs. Our design space is unique in that it is bidirectional in nature, as it examines how both the HMD can be used to enhance smartphone tasks, and how the smartphone can be used to enhance HMD tasks. We then present an interactive prototype that enables cross-device interactions across the proposed design space. A 12-participant user study demonstrates the promise of the design space and provides insights, observations, and guidance for the future.},
  booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
  pages = {1–14},
  numpages = {14},
  keywords = {augmented reality, cross-device computing, mixed-reality computing, smartphones},
  location = {Honolulu, HI, USA},
  series = {CHI '20},
  dimensions={true},
  pdf={BISHARE.pdf},
  video={https://www.youtube.com/watch?v=hkL4cKUleNw},
  selected={true},
  preview={BISHARE.gif},
}

@inproceedings{PhoneInVR,
  author = {Zhu, Fengyuan and Sousa, Mauricio and Sidenmark, Ludwig and Grossman, Tovi},
  title = {PhoneInVR: An Evaluation of Spatial Anchoring and Interaction Techniques for Smartphone Usage in Virtual Reality},
  year = {2024},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3613904.3642582},
  doi = {10.1145/3613904.3642582},
  abstract = {When users wear a virtual reality (VR) headset, they lose access to their smartphone and accompanying apps.
Past work has proposed smartphones as enhanced VR controllers, but little work has explored using existing smartphone apps and performing traditional smartphone interactions while in VR. 
In this paper, we consider three potential spatial anchorings for rendering smartphones in VR: On top of a tracked physical smartphone which the user holds (Phone-locked), on top of the users empty hand, as if holding a virtual smartphone (Hand-locked), or in a static position in front of the user (World-locked). 
We conducted a comparative study of target acquisition, swiping, and scrolling tasks across these anchorings using direct Touch or above-the-surface Pinch. 
Our findings indicate that physically holding a smartphone with Touch improves accuracy and speed for all tasks, and Pinch performed better with virtual smartphones. These findings provide a valuable foundation to enable smartphones in VR. },
  booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
  keywords = {virtual reality, cross-device computing, mixed-reality computing, smartphones},
  location = {Honolulu, HI, USA},
  series = {CHI '24},
  pdf={PhoneInVR.pdf},
  dimensions={true},
  video={https://www.youtube.com/watch?v=pQN98TbhAKM},
  selected={true},
  preview={PhoneInVR.gif},
}

@INPROCEEDINGS{VINS,
  author={Li, Peiliang and Qin, Tong and Hu, Botao and Zhu, Fengyuan and Shen, Shaojie},
  booktitle={2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Monocular Visual-Inertial State Estimation for Mobile Augmented Reality}, 
  year={2017},
  volume={},
  number={},
  pages={11-21},
  abstract={Mobile phones equipped with a monocular camera and an inertial measurement unit (IMU) are ideal platforms for augmented reality (AR) applications, but the lack of direct metric distance measurement and the existence of aggressive motions pose significant challenges on the localization of the AR device. In this work, we propose a tightly-coupled, optimization-based, monocular visual-inertial state estimation for robust camera localization in complex indoor and outdoor environments. Our approach does not require any artificial markers, and is able to recover the metric scale using the monocular camera setup. The whole system is capable of online initialization without relying on any assumptions about the environment. Our tightly-coupled formulation makes it naturally robust to aggressive motions. We develop a lightweight loop closure module that is tightly integrated with the state estimator to eliminate drift. The performance of our proposed method is demonstrated via comparison against state-of-the-art visual-inertial state estimators on public datasets and real-time AR applications on mobile devices. We release our implementation on mobile devices as open source software1.},
  pdf={https://ieeexplore.ieee.org/abstract/document/8115400},
  keywords={Cameras;Feature extraction;Mobile handsets;Measurement;State estimation;Visualization;Robustness},
  doi={10.1109/ISMAR.2017.18},
  preview={vins.png},
}

@inproceedings{Physhare,
author = {He, Zhenyi and Zhu, Fengyuan and Perlin, Ken},
title = {PhyShare: Sharing Physical Interaction in Virtual Reality},
year = {2017},
isbn = {9781450354196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131785.3131795},
doi = {10.1145/3131785.3131795},
abstract = {We present PhyShare, a new haptic user interface based on actuated robots. Virtual reality has recently been gaining wide adoption, and an effective haptic feedback in these scenarios can strongly support user's sensory in bridging virtual and physical world. Since participants do not directly observe these robotic proxies, we investigate the multiple mappings between physical robots and virtual proxies that can utilize the resources needed to provide a well rounded VR experience. PhyShare bots can act either as directly touchable objects or invisible carriers of physical objects, depending on different scenarios. They also support distributed collaboration, allowing remotely located VR collaborators to share the same physical feedback.},
booktitle = {Adjunct Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology},
pages = {17–19},
numpages = {3},
pdf={Physhare.pdf},
video={https://youtu.be/i3_3WPJAi4k?si=VgMbhVC5-FrNBwxY},
keywords = {virtual reality, robots, haptic user interfaces},
location = {Qu\'{e}bec City, QC, Canada},
series = {UIST '17 Adjunct}
}

@INPROCEEDINGS{TouchingTheDroid,
  author={Zhu, Fengyuan and Lyu, Zhuoyue and Sousa, Mauricio and Grossman, Tovi},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Touching The Droid: Understanding and Improving Touch Precision With Mobile Devices in Virtual Reality}, 
  year={2022},
  volume={},
  number={},
  abstract={Touch interaction with physical smartphones and tablets in Virtual Reality offers interesting opportunities for cross-device input. 
Unfortunately, any imprecision in the alignment of the visual representation of either the hand or device can impact the precision of touch and the realism of the experience. 
We first study a user`s ability to rely solely on preoperative feedback to perform touch interaction in VR, where no rendering of the hand is provided. 
Results indicate that touch in VR is possible without a visual representation of the hand, but accuracy is influenced by how the device is held and the distance traveled to the target. 
We then introduce a dynamic calibration algorithm to minimize the offset between the physical hand and its virtual representation. 
In a second study, we show that this algorithm can increase touch accuracy by 43\%, and minimize depth-based “screen penetration” or “floating touch” errors.},
  video={},
  preview={TouchTheDroid.gif},
  year={2022},
  location = {Singapore, Singapore},
  pdf={TouchTheDroid.pdf},
  pages={807-816},
  keywords={Performance evaluation;Visualization;Heuristic algorithms;User interfaces;Rendering (computer graphics);Calibration;Reliability;Human-Centered Computing;Human computer interaction (HCI);Interaction Paradigms;Virtual Reality},
  doi={10.1109/ISMAR55827.2022.00099}
}

@INPROCEEDINGS{PinchLens,
  author={Zhu, Fengyuan and Sidenmark, Ludwig and Sousa, Mauricio and Grossman, Tovi},
  booktitle={2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={PinchLens: Applying Spatial Magnification and Adaptive Control-Display Gain for Precise Selection in Virtual Reality}, 
  year={2023},
  volume={},
  number={},
  abstract ={We present PinchLens, a new free-hand target selection technique for acquiring small and dense targets in Virtual Reality. Traditional pinch-based selection does not allow people to precisely manipulate small and dense objects effectively due to tracking and perceptual inaccuracies. Our approach combines spatial magnification, an adaptive control-display gain, and visual feedback to improve selection accuracy. When a user starts the pinching selection process, a magnifying bubble expands the scale of nearby targets, an adaptive control-to-display ratio is applied to the user`s hand for precision, and a cursor is displayed at the estimated pinch point for enhanced visual feedback. We performed a user study to compare our technique to traditional pinch selection and several variations to isolate the impact of each of the technique’s features. The results showed that PinchLens significantly outperformed traditional pinch selection, reducing error rates from 18.9\% to 1.9\%.  Furthermore, we found that magnification was the dominant feature to produce this improvement, while the adaptive control-display gain and visual cursor of pinch were also helpful in several conditions.},
  pages={1221-1230},
  selected={true},
  location={Sydney, New South Wales, Australia},
  year={2023},
  pdf={PinchLens.pdf},
  video={https://youtu.be/MAlkiWVsMeQ?si=zPMXPQNwDKb0Yoc5},
  preview={PinchLens.gif},
  keywords={Visualization;Target tracking;Error analysis;Process control;Task analysis;Augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction techniques},
  doi={10.1109/ISMAR59233.2023.00139}
}

@inproceedings{b2b,
author = {Ma, Xiaojuan and Fang, Ke and Zhu, Fengyuan},
title = {From Breakage to Icebreaker: Inspiration for Designing Technological Support for Human-Human Interaction},
year = {2016},
isbn = {9781450340311},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901790.2901800},
doi = {10.1145/2901790.2901800},
abstract = {This paper explores why and how accidental breakage of technologies can promote humans to interact and ultimately lead to positive behavioral, emotional, and relational change. Through a set of research activities, including meta-synthesis of daily anecdotes, design workshops, and a case study, we gain insights into what may hinder or trigger human-human communication, and propose the conceptual and actionable process of Breakage-to-Icebreaker (B2I) design. Instead of intentionally breaking a technology, B2I design embeds mechanisms into existing products and services, creating opportunities for users to interpersonally interact online and/or offline while enjoying the original features and functionalities. Finally, we envision a broader and extended use of B2I thinking in everyday design research and practices.},
booktitle = {Proceedings of the 2016 ACM Conference on Designing Interactive Systems},
pages = {403–414},
pdf={b2b.pdf},
preview={dis16.png},
numpages = {12},
keywords = {icebreaker, human-human interaction, breakage},
location = {Brisbane, QLD, Australia},
series = {DIS '16}
}

